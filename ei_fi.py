# -*- coding: utf-8 -*-
"""EI_FI.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_U4GansIjIvGAae9lL_7xiVtD0FpdvZ7

# ***Imports***
"""

# Data Handling & Visualization
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

# Preprocessing
from sklearn.preprocessing import PolynomialFeatures, StandardScaler
from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV, KFold
from sklearn.metrics import mean_squared_error, r2_score

# Models
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from xgboost import XGBRegressor

# Utilities
from scipy.stats import randint
import shap
import joblib
import gradio as gr

"""# ***Data Handeling***"""

# Step 1: Load the dataset
file_path = "data_500_cl_wm_rt_eq.csv"  # Update path if necessary
data = pd.read_csv(file_path)

# Show the first few rows of the dataset
print("First 5 rows of the dataset:")
print(data.head())

# Step 2.1: Invert Reaction Time (so higher = better)
data['rt_inverted'] = 1 / (data['rt'] + 1e-6)
data.drop(columns=['rt'], inplace=True)
print(data.head())

# Select only the relevant numeric columns
features = ['cl', 'rt_inverted', 'wm', 'eq']

# Plot boxplots and calculate outliers
outlier_counts = {}

plt.figure(figsize=(14, 8))
for i, col in enumerate(features, 1):
    plt.subplot(2, 2, i)
    sns.boxplot(x=data[col], orient="h")
    plt.title(f'Boxplot of {col}')

    # IQR method
    Q1 = data[col].quantile(0.25)
    Q3 = data[col].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    outliers = data[(data[col] < lower_bound) | (data[col] > upper_bound)]
    outlier_counts[col] = len(outliers)

plt.tight_layout()
plt.show()

print("Outlier counts per feature:")
print(outlier_counts)

#Remove outliers

# Calculate IQR for 'cl'
Q1 = data['cl'].quantile(0.25)
Q3 = data['cl'].quantile(0.75)
IQR = Q3 - Q1

# Define bounds for outliers
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

# Print bounds (optional)
print(f"Lower Bound: {lower_bound}, Upper Bound: {upper_bound}")

# Filter out the outliers
data = data[(data['cl'] >= lower_bound) & (data['cl'] <= upper_bound)]

# Confirm removal
print("Updated shape of data:", data.shape)

# Select features
X_base = data[['cl', 'wm', 'rt_inverted']]

# Generate polynomial and interaction features up to degree 2
poly = PolynomialFeatures(degree=2, include_bias=False)
X_poly = poly.fit_transform(X_base)

# Get feature names for reference
feature_names = poly.get_feature_names_out(['cl', 'wm', 'rt_inverted'])

# Create a DataFrame for better readability
X_poly_df = pd.DataFrame(X_poly, columns=feature_names)

# Check the new feature set
print("Polynomial Feature Names:")
print(feature_names)

# Optionally, concatenate with the original target
X_poly_df['eq'] = data['eq'].values

print(X_poly_df.head())

# Plot all 9 feature distributions
plt.figure(figsize=(18, 12))

for idx, feature in enumerate(X_poly_df.columns):
    plt.subplot(4, 3, idx + 1)
    sns.histplot(X_poly_df[feature], bins=30, kde=True, color='cornflowerblue')
    plt.title(f'Distribution of {feature}')
    plt.xlabel(feature)
    plt.ylabel('Frequency')

plt.tight_layout()
plt.show()

#Standerdization
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_poly_df.drop(columns=['eq']))

# Convert back to DataFrame
X_scaled_df = pd.DataFrame(X_scaled, columns=X_poly_df.drop(columns=['eq']).columns)

# Preview scaled features
print("Scaled Features (first 5 rows):")
print(X_scaled_df.head())
Y=X_poly_df['eq']
print("\nTarget (first 5 rows):")
print(Y.head())

# Plot all 9 feature distributions
plt.figure(figsize=(18, 12))

for idx, feature in enumerate(X_scaled_df.columns):
    plt.subplot(4, 3, idx + 1)
    sns.histplot(X_scaled_df[feature], bins=30, kde=True, color='cornflowerblue')
    plt.title(f'Distribution of {feature}')
    plt.xlabel(feature)
    plt.ylabel('Frequency')

plt.tight_layout()
plt.show()

# Compute correlation matrix
corr = X_poly_df.corr()

# Plot
plt.figure(figsize=(12, 10))
sns.heatmap(corr, annot=True, fmt='.2f', cmap='coolwarm', square=True)
plt.title("Correlation Matrix")
plt.tight_layout()
plt.show()

# X_poly_df already includes all engineered features
X = X_scaled_df  # Features
y = Y                # Target

X_train, X_temp, y_train, y_temp = train_test_split(X, Y, test_size=0.30, random_state=42)

# Step 2: Then split temp into validation and test (15% each)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.50, random_state=42)

"""# ***RandomForestRegressor***"""

# Step 1: Define model
rf = RandomForestRegressor(random_state=42)

# Step 2: Define hyperparameter grid
param_grid = {
    'n_estimators': [100, 200, 300],         # number of trees
    'max_depth': [None, 5, 10, 15],          # depth of tree
    'min_samples_split': [2, 5, 10],         # min samples to split
    'min_samples_leaf': [1, 2, 4],           # min samples per leaf
    'max_features': ['sqrt', 'log2']         # number of features to consider at each split
}

# Step 3: Set up K-Fold CV
kfold = KFold(n_splits=5, shuffle=True, random_state=42)

# Step 4: Grid Search with CV
rf_grid_search = GridSearchCV(estimator=rf,
                           param_grid=param_grid,
                           scoring='neg_mean_squared_error',  # since we want to reduce MSE
                           cv=kfold,
                           n_jobs=-1,
                           verbose=2)

# Step 5: Fit to your training data
rf_grid_search.fit(X_train, y_train)

# Step 6: Get best model and evaluate
best_rf = rf_grid_search.best_estimator_
y_val_pred = best_rf.predict(X_val)

# Step 7: Metrics
r2_val = r2_score(y_val, y_val_pred)
mse_val = mean_squared_error(y_val, y_val_pred)

print("âœ… Best Parameters:", rf_grid_search.best_params_)
print(f"ðŸŽ¯ RÂ² Score (Best RF): {r2_val}")
print(f"ðŸ“‰ MSE (Best RF): {mse_val}")

# Define the model
rf = RandomForestRegressor(random_state=42)

# Extended search space
param_dist_rf = {
    'n_estimators': randint(100, 500),
    'max_depth': [None] + list(range(5, 30, 5)),
    'min_samples_split': randint(2, 10),
    'min_samples_leaf': randint(1, 7),
    'max_features': ['sqrt', 'log2', None]
}

kfold = KFold(n_splits=5, shuffle=True, random_state=42)

# Randomized Search
random_search_rf = RandomizedSearchCV(
    rf,
    param_distributions=param_dist_rf,
    n_iter=100,  # Try 100 combinations
    scoring='neg_mean_squared_error',
    cv=kfold,
    random_state=42,
    n_jobs=-1,
    verbose=2
)

random_search_rf.fit(X_train, y_train)

print("âœ… Best RF Params:", random_search_rf.best_params_)
y_pred_rf = random_search_rf.best_estimator_.predict(X_val)
print("ðŸŽ¯ RÂ² (RF):", r2_score(y_val, y_pred_rf))
print("ðŸ“‰ MSE (RF):", mean_squared_error(y_val, y_pred_rf))

"""# ***GradientBoostingRegressor***"""

# Step 1: Define the model
gbr = GradientBoostingRegressor(random_state=42)

# Step 2: Define hyperparameter grid
param_grid = {
    'n_estimators': [100, 200, 300],
    'learning_rate': [0.01, 0.05, 0.1],
    'max_depth': [3, 5, 7],
    'min_samples_split': [2, 5],
    'min_samples_leaf': [1, 2]
}

# Step 3: Define cross-validation strategy
kfold = KFold(n_splits=5, shuffle=True, random_state=42)

# Step 4: Grid search with cross-validation
gb_grid_search = GridSearchCV(
    estimator=gbr,
    param_grid=param_grid,
    scoring='neg_mean_squared_error',  # because we want to minimize MSE
    cv=kfold,
    n_jobs=-1,
    verbose=2
)

# Step 5: Fit on training data only
gb_grid_search.fit(X_train, y_train)

# Step 6: Evaluate on test set
best_gbr = gb_grid_search.best_estimator_
y_val_pred = best_gbr.predict(X_val)

# Step 7: Print metrics
r2 = r2_score(y_val, y_val_pred)
mse = mean_squared_error(y_val, y_val_pred)

print("âœ… Best Parameters:", gb_grid_search.best_params_)
print(f"ðŸŽ¯ RÂ² Score (Best GBR): {r2}")
print(f"ðŸ“‰ MSE (Best GBR): {mse}")

#Define the Model
gbr = GradientBoostingRegressor(random_state=42)

#Extended Search Space
param_dist_gbr = {
    'n_estimators': randint(100, 500),
    'max_depth': range(3, 10),
    'learning_rate': [0.01, 0.05, 0.1, 0.2],
    'min_samples_split': randint(2, 10),
    'min_samples_leaf': randint(1, 5)
}

#Randomized Search
random_search_gbr = RandomizedSearchCV(
    gbr,
    param_distributions=param_dist_gbr,
    n_iter=100,
    scoring='neg_mean_squared_error',
    cv=5,
    random_state=42,
    n_jobs=-1,
    verbose=2
)

random_search_gbr.fit(X_train, y_train)

print("âœ… Best GBR Params:", random_search_gbr.best_params_)
y_pred_gbr = random_search_gbr.best_estimator_.predict(X_val)
print("ðŸŽ¯ RÂ² (GBR):", r2_score(y_val, y_pred_gbr))
print("ðŸ“‰ MSE (GBR):", mean_squared_error(y_val, y_pred_gbr))

"""# ***XGBRegressor***"""

# Step 1: Define the model
xgb = XGBRegressor(objective='reg:squarederror', random_state=42)

# Step 2: Define hyperparameter grid
param_grid = {
    'n_estimators': [100, 200, 300],
    'learning_rate': [0.01, 0.05, 0.1],
    'max_depth': [3, 5, 7],
    'subsample': [0.7, 1.0],
    'colsample_bytree': [0.7, 1.0]
}
# Step 3: Define cross-validation strategy
cv_strategy = KFold(n_splits=5, shuffle=True, random_state=42)

# Step 4: Grid search with cross-validation
xg_grid_search = GridSearchCV(
    estimator=xgb,
    param_grid=param_grid,
    scoring='neg_mean_squared_error',
    cv=cv_strategy,
    n_jobs=-1,
    verbose=2
)

# Step 5: Fit on training data only
xg_grid_search.fit(X_train, y_train)

# Step 5: Fit on training data only
best_xgb = xg_grid_search.best_estimator_
y_val_pred = best_xgb.predict(X_val)

# Step 7: Print metrics
r2 = r2_score(y_val, y_val_pred)
mse = mean_squared_error(y_val, y_val_pred)

print("âœ… Best Parameters:", xg_grid_search.best_params_)
print(f"ðŸŽ¯ RÂ² Score (Best XGB): {r2}")
print(f"ðŸ“‰ MSE (Best XGB): {mse}")

#define model
xgb = XGBRegressor(random_state=42)

#Extended Search Space
param_dist_xgb = {
    'n_estimators': randint(100, 500),
    'max_depth': range(3, 15),
    'learning_rate': [0.01, 0.05, 0.1, 0.2],
    'subsample': [0.6, 0.7, 0.8, 1.0],
    'colsample_bytree': [0.6, 0.8, 1.0]
}

#Randomized Search
random_search_xgb = RandomizedSearchCV(
    xgb,
    param_distributions=param_dist_xgb,
    n_iter=100,
    scoring='neg_mean_squared_error',
    cv=5,
    random_state=42,
    n_jobs=-1,
    verbose=2
)

random_search_xgb.fit(X_train, y_train)

print("âœ… Best XGB Params:", random_search_xgb.best_params_)
y_pred_xgb = random_search_xgb.best_estimator_.predict(X_val)
print("ðŸŽ¯ RÂ² (XGB):", r2_score(y_val, y_pred_xgb))
print("ðŸ“‰ MSE (XGB):", mean_squared_error(y_val, y_pred_xgb))

"""# ***Feature Importance (Random Forest)***"""

# Get the best model from RandomizedSearchCV
best_rf = rf_grid_search.best_estimator_

# Get feature importances
importances = best_rf.feature_importances_
feature_names = X_train.columns  # or use poly.get_feature_names_out([...]) if using PolynomialFeatures

# Create DataFrame for easier plotting
feat_imp_df = pd.DataFrame({
    'Feature': feature_names,
    'Importance': importances
}).sort_values(by='Importance', ascending=False)

# Plot
plt.figure(figsize=(10, 6))
sns.barplot(x='Importance', y='Feature', data=feat_imp_df, palette='Blues_d')
plt.title('Feature Importances - Random Forest')
plt.tight_layout()
plt.show()

"""# ***SHAP and Explainer***"""

# Use TreeExplainer for tree-based models
explainer = shap.TreeExplainer(best_rf)  # Replace `best_rf` with your best model

# You can use a subset of your data for faster results
shap_values = explainer.shap_values(X_val)

shap.summary_plot(shap_values, X_val, plot_type="bar")  # Bar plot of average importance
shap.summary_plot(shap_values, X_val)  # Beeswarm plot for distribution of impacts

"""# ***Testing***"""

# Predict on test set using the best model (e.g., best_rf from RandomizedSearchCV)
y_test_pred = rf_grid_search.best_estimator_.predict(X_test)

# Metrics
test_r2 = r2_score(y_test, y_test_pred)
test_mse = mean_squared_error(y_test, y_test_pred)

print("ðŸŽ¯ RÂ² Score on Test Set:", test_r2)
print("ðŸ“‰ MSE on Test Set:", test_mse)

"""# ***PLOTS***"""

# Line Plot
plt.figure(figsize=(30, 6))
plt.plot(range(len(y_test)), y_test, marker='o', color='blue', label='Test Data')
plt.plot(range(len(y_test_pred)), y_test_pred, marker='x', color='red', label='Predicted Data')
plt.xlabel('Data Point')
plt.ylabel('Values')
plt.title('Comparison of Test and Predicted Data (Original Scale)')
plt.legend()
plt.grid(True)
plt.show()

#Distribution of Prediction Errors
errors = y_test - y_test_pred
plt.figure(figsize=(8, 5))
sns.histplot(errors, bins=30, kde=True, color='slateblue')
plt.title('Distribution of Prediction Errors')
plt.xlabel('Error')
plt.ylabel('Frequency')
plt.grid(True)
plt.show()

"""# ***DEMO***"""

# Save the model and preprocessing steps
best_rf = rf_grid_search.best_estimator_
joblib.dump(best_rf, "best_rf_model.joblib")
joblib.dump(scaler, "scaler.joblib")
joblib.dump(poly, "poly.joblib")

# Load the saved components
model = joblib.load("best_rf_model.joblib")
scaler = joblib.load("scaler.joblib")
poly = joblib.load("poly.joblib")  # this is your PolynomialFeatures

# Define the prediction function
def predict_eq(cl, wm, rt):
    # Invert RT as done in preprocessing
    rt_inverted = 1 / (rt + 1e-6)

    # Create input array
    input_features = np.array([[cl, wm, rt_inverted]])

    # Polynomial transformation
    input_poly = poly.transform(input_features)

    # Standardize
    input_scaled = scaler.transform(input_poly)

    # Predict
    prediction = model.predict(input_scaled)[0]
    return round(prediction, 3)

# Set up Gradio interface
interface = gr.Interface(
    fn=predict_eq,
    inputs=[
        gr.Number(label="Cognitive Load (cl)"),
        gr.Number(label="Working Memory (wm)"),
        gr.Number(label="Reaction Time (rt)"),
    ],
    outputs=gr.Number(label="Predicted Emotional Intelligence (eq)"),
    title="EI Predictor",
    description="Predict Emotional Intelligence based on Cognitive Load, Working Memory, and Reaction Time"
)

# Launch the app
interface.launch()